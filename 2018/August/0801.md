
**2018-08-01**

# Active Learning

[1] 《Active Learning by Learning》

Wei-Ning Hsu, Hsuan-Tien Lin

Abstract: Pool-based active learning is an important technique that helps reduce labeling efforts within a pool of unlabeled instances. Currently, most pool-based active learning strategies are constructed based on some human-designed philosophy; that is, they reflect what human beings assume to be “good labeling questions.” However, while such human-designed philosophies can be useful on specific data sets, it is often difficult to establish the theoretical connection of those philosophies to the true learning performance of interest. In addition, given that a single human-designed philosophy is unlikely to work on all scenarios, choosing and blending those strategies under different scenarios is an important but challenging practical task. This paper tackles this task by letting the machines adaptively “learn” from the performance of a set of given strategies on a particular data set. More specifically, we design a learning algorithm that connects active learning with the well-known multi-armed bandit problem. Further, we postulate that, given an appropriate choice for the multi-armed bandit learner, it is possible to estimate the performance of different strategies on the fly. Extensive empirical studies of the resulting ALBL algorithm confirm that it performs better than state-of-the-art strategies and a leading blending algorithm for active learning, all of which are based on human-designed philosophy.

My thoughts: Most existing active learning algorithms are human designed with some heuristics, this work associates the multi-armed bandit problem with the combined active learning problem. They want the combined model to learn adaptively to choose one human-designed heuristic after each query to achieve higher performance on different data sets. The major work in associate multi-armed bandit problem to this work is designing a good reward function so that model can update properly, they use an unbiased sampling of the current labeled points and treat this set as an unbiased estimation of the test set, then they use the loss of the estimated test set as the final reward. As for the strength of this paper: 

1. The idea of formulating combined active learning algorithms as multi armed bandit problems is interesting.
2. The deisign of reward function is reasonable.
3. The experiments results are also convincing.

However, there are some points that I have concern about this paper: 

1. Can this algorithm gurantee its perfromance will always be better than its children algorithms? It seems like the composition of the children algorithms matters: e.g one "clever" with several "normal" algorithms, will the proposed algorithm continues agree on the "normal" algortihms' selection thus putting more weights on them? 

2. Sometimes the training set is biased labeled, although we can use unlabeled data to help us do unbiased sampling, but the initial bias is still pretty large. How to deal with this case?

Paper: http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9636/9924
