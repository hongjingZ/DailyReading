
**2018-08-07**

# Deep Spectral Clustering Learning

[1] 《Deep Spectral Clustering Learning》

Marc T. Law, Raquel Urtasun, Richard S. Zemel 

Abstract: Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is fixed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is efficient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets confirm state-of-the-art Recall @K performance.

Summary: The paper introduces a deep learning based approach that leanrs a transformed metric to maximize the supervised clustering results. The basic idea is to train a neural network to map a representation of input to maximize the clustering assignments. The method uses several relaxations and mathematical tricks on Kmeans problem to derive an approximate solution. The final formulation can be solved via gradient descent in closed form. Once the metric is learned, the paper also propose a "spectral clustering style" work which partitions a dataset by exploiting the leading eigenvectors of a similarity matrix derived from their learned metric. Experiments on images dataset show that using their learned metric in "spectral clustering" way achieves state of the art performance based on Recall@K performance. 

Main Contributions:

1.  Based on Kmeans algorithm, they use constraint relaxation to derive a simple supervised clustering formulation which approximates the Kmeans(can be solved directly, no longer NP-hard).

2.  They solve the optimization problem in closed-form by gradient descent with minor assumptions. Moreover the solving complexity is pretty low comparing to related works.

3.  They connect deep neural networks with this supervised clustering problem, the network's output is the desired feature embedding(desired metric).The network is trained through backpropagating the closed-form gradient which optimized for satisfying the supervised clustering results.

4.  They apply learned metric into "spectral clustering style" clustering and achieve state of the art Recall@K performance in several image datasets.

paper: http://proceedings.mlr.press/v70/law17a.html


