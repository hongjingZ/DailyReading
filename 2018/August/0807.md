
**2018-08-07**

# Deep Spectral Clustering Learning

[1] 《Deep Spectral Clustering Learning》

Marc T. Law, Raquel Urtasun, Richard S. Zemel 

Abstract: Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is fixed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is efficient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets confirm state-of-the-art Recall @K performance.

Summary: The paper introduces a deep learning based approach that leanrs a transformed metric to maximize the supervised clustering results. The basic idea is to train a neural network to map a representation of input to maximize the clustering assignments. The method uses several relaxations and mathematical tricks on Kmeans problem to derive an approximate solution. The final formulation can be solved via gradient descent in closed form. Once the metric is learned, the paper also propose a "spectral clustering style" work which partitions a dataset by exploiting the leading eigenvectors of a similarity matrix derived from their learned metric. Experiments on images dataset show that using their learned metric in "spectral clustering" way achieves state of the art performance based on Recall@K performance. 

Main Contributions:

1.  Based on Kmeans algorithm, they use constraint relaxation to derive a simple supervised clustering formulation which approximates the Kmeans(can be solved directly, no longer NP-hard).

2.  They solve the optimization problem in closed-form by gradient descent with minor assumptions. Moreover the solving complexity is pretty low comparing to related works.

3.  They connect deep neural networks with this supervised clustering problem, the network's output is the desired feature embedding(desired metric).The network is trained through backpropagating the closed-form gradient which optimized for satisfying the supervised clustering results.

4.  They apply learned metric into "spectral clustering style" clustering and achieve state of the art Recall@K performance in several image datasets.

paper: http://proceedings.mlr.press/v70/law17a.html

[2] 《SpectralNet: Spectral Clustering using Deep Neural Networks》

Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri, Yuval Kluger

Abstract: Spectral clustering is a leading and popular technique in unsupervised data analysis. Two of its major limitations are scalability and generalization of the spectral embedding (i.e., out-of-sample-extension). In this paper we introduce a deep learning approach to spectral clustering that overcomes the above shortcomings. Our network, which we call SpectralNet, learns a map that embeds input data points into the eigenspace of their associated graph Laplacian matrix and subsequently clusters them. We train SpectralNet using a procedure that involves constrained stochastic optimization. Stochastic optimization allows it to scale to large datasets, while the constraints, which are implemented using a specialpurpose output layer, allow us to keep the network output orthogonal. Moreover,
the map learned by SpectralNet naturally generalizes the spectral embedding to unseen data points. To further improve the quality of the clustering, we replace the standard pairwise Gaussian affinities with affinities learned from the given unlabeled data using a Siamese network. Additional improvement of the resulting clustering can be achieved by applying the network to code representations produced, e.g., by standard autoencoders. Our end-to-end learning procedure is fully unsupervised. In addition, we apply VC dimension theory to derive a lower bound on the size of SpectralNet. State-of-the-art clustering results are reported on the Reuters dataset. Our implementation is publicly available at https://github.com/kstant0725/SpectralNet.

Summary: The paper introduces a deep learning based approach that approximate spectral clustering. The basic idea is to train a neural network to map a representation of input in the eigenspace where k-means clustering can be performed as usual. The method is more scalable than the vanilla spectral clustering algorithm and it can also be used to cluster a new incoming data point without doing the whole spectral clustering procedure(The network directly mapping the input to k eigenvectors). Moreover the authors have proved worst case lower bounds on the size of neural network required to perfrom the task using VC dimension theory. The experiments on MNIST and Reuters dataset show state of the art performance. 

Main Contributions:

1.  Design a novel neural network which maps the input points to their embeddings in the eigenspace.

2.  Use constraint optimization to train the final layer of neural network(which output the embedding) to make sure the ourput "eigenvectors" are orthonormal

3.  Use stochastic optimization on mini-batches, solve the scalability problem.

4.  Theory prove for VC dimension of spectral clustering.

5.  Replace the Gaussian affinity measure with Siamese neural network(trained in an unsupervised way) to get better affinity measure. Which demonstrate good results in experiments.

Potential human guidance work based on SpecralNet:

1.  The unsupervised training of Siamese is based on code k-nearest neighbor approach to get positive and negative examples, although the experiments show this way is better than Gaussian kernel but it's not clear why. One way to boost the performance of Siamese Network could be adding pairwise constraints(must-link, cannot link) to help the Siamese network to achive better encoding.

2.  To enforce the output "eigenvectors" of spectralNet to be orthonormal. this work use choleskly decomposition to train the final layer of neural network. Similarly, we can introduce other constraints to this layer's output, like the work in flexible constrained clustering which uses relaxed "must-link" and "cannot-link" in the spectral clustering optimizing process, we can also design a way to enforce these constraints into a neural network's layer.

3.  Neural networks have large scalabilty, to cope with this capability, we can design group guidance which assigns the purity degree of a batch of learning examples. This can be seen as a weak guidance with higher scalability.

4.  Once the network is learned, we can use the output embedding as latent variable, plus the original data to train a conditional GAN which conditioned on these latent variables. Once we derive the learned GAN, we can interprete all these embeddings with the visualization of the GAN's generated results. 

Paper: https://openreview.net/pdf?id=HJ_aoCyRZ
