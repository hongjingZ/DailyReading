
**2018-07-25/07-26**

# GAN Survey(Image Synthesis)

[1] 《An Introduction to Image Synthesis with Generative Adversarial Nets》

He Huang, Phillip S. Yu and Changhu Wang

Abstract:  There has been a drastic growth of research in Generative Adversarial Nets (GANs) in the past few years. Proposed in 2014, GAN has been applied to various applications such as computer vision and natural language processing, and achieves impressive performance. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.

This is a good survey paper, I will go through all the recent works within the paper. 

GAN PRELIMINARIES:
1. Conditional GAN: Typically, the conditional input vector c is concatenated with the noise vector z, and the resulting vector is put into the generator as it is in the original GAN.
2. ACGAN: In order to feed more side-information and to allow for semi-supervised learning, one can add an additional taskspecific auxiliary classifier to the discriminator, so that the model is optimized on the original tasks as well as the additional task.
3. GAN with Encoder: If we treat the noise distribution as a latent feature space for data samples, GAN lacks the ability to map
data sample x into latent feature z. In order to allow such mapping, two concurrent works BiGAN and ALI propose to add an encoder E in the original GAN framework.
4. GAN with Variational Auto-Encoder: VAE-GAN proposes to combine Variational AutoEncoder (VAE) with GAN to exploit both of their benefits, as GAN can generate sharp images but often miss some modes while images produced by VAE are blurry but have large variety.
5. Handling Mode Collapse: In the original GAN formulation, the discriminator does not need to consider the variety of synthetic samples, but only focuses on telling whether each sample is realistic or not, which makes it possible for the generator to spend efforts in generating a few samples that are good enough to fool the discriminator. For example, although the MNIST dataset contains images of digits from 0 to 9, in an extreme case, a generator only needs to learn to generate one of the ten digits perfectly to completely fool the discriminator, and then the generator stops trying to generate the other nine digits. The absence of the other nine digits is an example of inter-class mode collapse. An example of intra-class mode collapse is, there are many writing styles for each of the
digits, but the generator only learns to generate one perfect sample for each digit to successfully fool the discriminator. Solutions: Minibatch features, MRGAN, BiGAN, WGAN, WGAN-GP. 

GENERAL APPROACHES OF IMAGE SYNTHESIS WITH GAN:

<p align="center"><img width="90%" src="image_synthesis.png" /></p>

1.  Direct Methods:  Many of the earliest GAN models fall into this category, like GAN, DCGAN, ImprovedGAN, InfoGAN, f-GAN and GANINTCLS. Among them, DCGAN is one of the most classic ones whose structure is used by many later models. Direct methods are easy to implement and straightfoward when compared with hierarchical and iterative methods, and it usually achieves good results.

2. Hierarchical Methods: Contrary to the Direct Method, algorithms under the Hierarchical Method use two generators and two discriminators in their models, where different generators have different purposes. The idea behind those methods is to separate an image into two parts, like “styles & structure” and “foreground & background”. The relation between the two generators may be either parallel or sequential. Representative works: SS-GAN, LR-GAN

3. Iterative Methods: This method differentiates itself from Hierarchical Methods in two ways. First, instead of using two different generators that perform different roles, the models in this category use multiple generators that have similar or even the same structures, and they generate images from coarse to fine, with each generator refining the details of the results from the previous generator. Second, when using the same structures in the generators, Iterative Methods can use weight-sharing among the generators, while Hierarchical Methods usually can not. Representative works: LAPGAN, StackGAN, SGAN, GRAN.

TEXT-TO-IMAGE SYNTHESIS:
When we apply GAN to image synthesis, it is desired to control the content of generated images. Although there are label-conditioned GAN models like cGAN which can generate images belong to a specific class, it remains a great challenge to generate images based on text descriptions. Text-to-image synthesis is kind of the holy grail of computer vision, since if an algorithm can generate truly realistic
images from mere text descriptions, we can have a high confidence that the algorithm actually understands what is in the images, where computer vision is about teaching computers to see and understand visual contents in the real world.

1. Text-to-Image with Location Constraints: Although GAN-INT-CLS and StackGAN can generate images based on text description, they fail to capture the localization constraints of the objects in images. To allow encoding spatial constraints, Reed et al. propose GAWWN that presents two possible solutions.The main benefit of specifying the locations of each part of the objects is that it yields more interpretable results, and that it is desirable that the model can understand the concepts of different parts of objects, which is one of the ultimate goals of computer vision.

2. Text-to-Image with Stacked GANs: StackGAN, StackGAN++, AttnGAN.

3. Text-to-Image by Iterative Sampling: Different from previous approaches that directly incorporate the text information in the generation process, PPGN proposes to use activation maximization method to generate images in an iterative sampling way. The model contains two separate parts: a pretrained image captioning model, and an image generator.

4. Limitations of Current Text-to-Image Models: Generative Adversarial Network certainly provides us a promising way to do text-to-image synthesis, since it produces sharper images than any other generative methods so far. To take a further step in text-to-image synthesis, we need to figure out novel ways to teach the algorithms theconcepts of things. One possible way is to train separate models that can generate different kinds of objects, and then train another model that learns how to combine different objects (i.e. the reasonable relations between objects) into one image based on the text descriptions. However, such method requires large training sets for different objects, and another large dataset that contains images of those different objects, which is hard to acquire. Another possible direction may be to make use of the Capsule idea proposed by Hinton et al. since capsules are designed to capture the concepts of objects, but how to efficiently train such capsulebased network is still a problem to be solved.

IMAGE-TO-IMAGE TRANSLATION: We introduce several models that work for general image-to-image translation, from supervised methods to unsupervised ones. The “supervision” here means that for each image in the source domain, there is a corresponding ground-truth image in
the target domain. Later we will turn into different specific applications such as face editing, image super resolution, image painting and video prediction.

1. Supervised Image-to-Image Translation: Pix2Pix proposes to combine the loss of a conditional Generative Adversarial Network (cGAN) with L1 regularization loss, so that the generator is not only trained to fool the discriminator but also to generate images as close to
ground-truth as possible. The reason for using L1 instead of L2 is that L1 produces less blurry images.

2. Supervised Image-to-Image Translation with Pairwise Discrimination: PLDT proposes another method to do supervised imageto-image
translation, by adding another discriminator Dpair that learns to tell whether a pair of images from different domains is associated with each other. 

3. Unsupervised Image-to-Image Translation with Cyclic Loss:

<p align="center"><img width="90%" src="CycleGAN.png" /></p>

4. Unsupervised Image-to-Image Translation with Distance Constraint: DistanceGAN discovers that, the distance ||xi − xj || between two images in the source domain A is highly positively correlated to the distance of their counterparts ||GAB(xi)−GAB(xj )|| in the target domain B, which can be seen from Figure 1 of the paper.

5. Unsupervised Image-to-Image Translation with Feature Constancy: As we mention previously, besides minimizing the reconstruction error at raw pixel level, we can also do this at higher feature level, which is explored in DTN. Experiments show that DTN produces impressive
images on face-to-emoji task, which is competitive with some existing emoji generating programs.

6. Unsupervised Image-to-Image Translation with Auxiliary Classifier:

7. Unsupervised Image-to-Image Translation with VAE and Weight Sharing: 

8. Unsupervised Multi-domain Image-to-Image Translation:

9. Summary on General Image-to-Image Translation:

<p align="center"><img width="90%" src="imgSummary.png" /></p>

EVALUATION METRICS ON SYNTHETIC IMAGES:

1. A commonly used subjective metric is to use the Amazon Mechanical Turk that hires humans to score synthetic and real images according to how realistic they think the images are. However, people often have different opinions of what is good or bad, so we also need objective metrics to evaluate the quality of images.

2. Inception score (IS) evaluates an image based on the entropy in class probability distribution when it is put into a pre-trained image classifier. 

3. Similar to Inception score, FCN-score adopts the idea that if the synthetic images are realistic, classifiers trained on real images will be able to classify the synthetic images correctly.

4. FID. The authors of this work show that FID is consistent with human judgment and that there is a strong negative correlation between FID and the quality of generated images. Furthermore, FID is less sensitive to noise than IS and can detect intra-class mode collapse.

DISCRIMINATORS AS LEARNED LOSS FUNCTIONS:

My Thoughts:

1. For text-to-image synthesis:

2. For image-to-image translation:

Paper: https://arxiv.org/pdf/1803.04469.pdf
