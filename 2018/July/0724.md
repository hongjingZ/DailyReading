
**2018-07-24**

# Siamese Network

[1] 《Learning a similarity metric discriminatively, with application to face verification》

Sumit Chopra, Raia Hadsell, Yann LeCun

Abstract: We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the norm in the target space approximates the “semantic” distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.

Network Structure:

<p align="center"><img width="60%" src="Siamese_arch.png" /></p>

Conclusion:

We present a general discriminative method for learning complex similarity metrics. The method is best suited for classification or verification scenarios where the number of classes is very large, and/or where examples of all the classes are not available at the time of training. We illustrate the method with a face verification application.

Paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1467314

Code:  https://github.com/Shicoder/DeepLearning_Demo/tree/master/siamese_tf_mnist

# GAN

[2] 《Improved Techniques for Training GANs》

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen

Abstract: We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.

Convergent GAN training:
1. Feature matching addresses the instability of GANs by specifying a new objective for the generator that prevents it from overtraining on the current discriminator.
2. Minibatch discriminationn allows us to generate visually appealing samples very quickly, and in this regard it is superior to feature matching.
3. Historical averaging.
4. One-sided label smoothing.
5. Virtual batch normalization.

Assesment of image quality: 
1. Human annotators from MTurk.
2. Inception score. Apply the Inception model to every generated image to get the conditional label distribution p(y|x). Images that contain meaningful objects should have a conditional label distribution p(y|x) with low entropy. Moreover, we expect the model to generate varied images, so the marginal integration p(y|x = G(z))dz should have high entropy. The metric that they propose is: exp(ExKL(p(y|x)||p(y))).

GAN for semi-supervised learning:

<p align="center"><img width="70%" src="ssgan.png" /></p>

Paper: https://arxiv.org/abs/1606.03498

# Active Learning

[3] 《Learning Active Learning from Data》

Ksenia Konyushkova, Sznitman Raphael, Pascal Fua

Abstract: In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can be learnt either from simple synthetic 2D datasets or from a subset of domain-specific data. Our method yields strategies that work well on real data from a wide range of domains.

Monte Carlo LAL:
Our approach to AL is data-driven and can be formulated as a regression problem. Given a representative dataset with ground truth, we simulate an online learning procedure using a Monte-Carlo technique. We propose two versions of AL strategies that differ in the way how datasets for learning a regressor are constructed. When building the first one, LALINDEPENDENT, we incorporate unused labels individually and at random to retrain the classifier. Our goal is to correlate the change in test performance with the properties of the classifier and of newly added datapoint. To build the LALITERATIVE strategy, we further extend our method by a sequential procedure to account for selection bias caused by AL. We formalize our LAL procedures in the remainder of the section.

My thoughts: This paper provides us another novel way to do active learning, instead of designing heuristics for specific learning settings, they formulate the AL as a regression problem. They select query instances based on the model attributes and the instance attributes. This work uses transfer learning's idea which transfer the model attributes learned from synthetic dataset into the real world application models. This is an interesting idea. The experiments results demonstrate learning active learning from 2D data generalizes well to many real domain applications.

Paper: https://papers.nips.cc/paper/7010-learning-active-learning-from-data.pdf





